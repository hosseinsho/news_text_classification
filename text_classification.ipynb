{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "text_classification.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip3 install transformers sentencepiece hazm clean-text[gpl]\n",
    "!pip install pyyaml==5.4.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!gdown 1D3yt99D0GcCRCbdKbUQGxbqjkeh91hTg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!unrar x hamshahri.rar\n",
    "!cp /content/hamshahriold/Corpus/Hamshahri-Categories.txt /content/\n",
    "!unzip /content/hamshahriold/Corpus/Hamshahri-Corpus.zip\n",
    "!unzip /content/hamshahriold/Corpus/PersianStopWords.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import hazm\n",
    "from transformers import AutoConfig, AutoTokenizer, TFAutoModel, AutoModel, DataCollatorWithPadding\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save data to csv file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [[DID value, Date value, CAT, text]]\n",
    "corpus = []\n",
    "tmp_text = \" \"\n",
    "tmp_values = []\n",
    "c = 0\n",
    "with open('Hamshahri-Corpus.txt', \"rb\") as file:\n",
    "  for line in file:\n",
    "    line = line.decode(\"UTF-8\")\n",
    "    if \".DID\" in line:\n",
    "      # some news are abnormal lenght and they are low in number(about 1000)\n",
    "      if len(tmp_text.split(' ')) < 2500:\n",
    "        tmp_values.append(tmp_text)\n",
    "        corpus.append(tmp_values)\n",
    "      tmp_text = \"\"\n",
    "      tmp_values = []\n",
    "      tmp_values.append(line.replace(\".DID\\t\", \"\").replace(\"\\r\\n\",\"\"))\n",
    "    elif \".Date\" in line:\n",
    "      tmp_values.append(line.replace(\".Date\\t\", \"\").replace(\"\\r\\n\",\"\").replace(\"\\\\\", \"/\"))\n",
    "    elif \".Cat\" in line:\n",
    "      tmp_values.append(line.replace(\".Cat\\t\", \"\").replace(\"\\r\\n\",\"\"))\n",
    "    else:\n",
    "      tmp_text += (line.strip() + \" \")\n",
    "corpus.pop(0)\n",
    "len(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(corpus, columns=['DID', 'date', 'cat', 'text'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(\"dataset.csv\", date_format='%Y%m%d')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df[['text', 'cat']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = 0\n",
    "y = 0\n",
    "z = 0\n",
    "for txt in df[\"text\"]:\n",
    "  l = len(txt.split(\" \"))\n",
    "  if len(txt.split(\" \")) > 2500:\n",
    "    z += 1\n",
    "    x = l\n",
    "    y=txt\n",
    "x\n",
    "z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# stop word\n",
    "stop_words_list = []\n",
    "with open('PersianStopWords.txt', \"rb\") as file:\n",
    "  for line in file:\n",
    "    stop_words_list.append(line.decode(\"UTF-8\").replace('\\r\\n', \"\"))\n",
    "\n",
    "for idx, txt in enumerate(df[\"text\"]):\n",
    "  word_tokenized =  hazm.word_tokenize(txt)\n",
    "  cps = \"\"\n",
    "  for word in word_tokenized:\n",
    "    if word not in stop_words_list:\n",
    "      cps += word + \" \"\n",
    "      \n",
    "  df.loc[idx].at['text'] = cps\n",
    "  if idx % 30000 == 0:\n",
    "    print(idx, \"numbers cleaned\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalization\n",
    "The text have different lengths based on words! Detecting the most normal range could help us find the maximum length of the sequences for the preprocessing step"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate the length of text based on their words\n",
    "df['text_len_by_words'] = df['text'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "min_max_len = df[\"text_len_by_words\"].min(), df[\"text_len_by_words\"].max()\n",
    "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_gl_than(data, less_than=100.0, greater_than=0.0, col='text_len_by_words'):\n",
    "    data_length = data[col].values\n",
    "    data_glt = sum([1 for length in data_length if greater_than < length <= less_than])\n",
    "    data_glt_rate = (data_glt / len(data_length)) * 100\n",
    "    print(f'Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "minlim, maxlim = 10, 1000\n",
    "data_gl_than(df, maxlim, minlim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove text with the length of fewer than minlim words and more than maxlim\n",
    "df['text_len_by_words'] = df['text_len_by_words'].apply(lambda len_t: len_t if minlim <= len_t <= maxlim else None)\n",
    "df = df.dropna(subset=['text_len_by_words'])\n",
    "df = df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df['text_len_by_words']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of word counts within text',\n",
    "    xaxis_title_text='Word Count',\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.2)\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "groupby_cat = df.groupby('cat')['cat'].count()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(groupby_cat.index),\n",
    "    y=groupby_cat.tolist(),\n",
    "    text=groupby_cat.tolist(),\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of rate within text',\n",
    "    xaxis_title_text='Rate',\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.2,\n",
    "    bargroupgap=0.2)\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### balance data which their cats are under 1000 instances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_cats = list(groupby_cat.index)\n",
    "group_values = list(groupby_cat.values)\n",
    "remove_cats = []\n",
    "for idx, cat in enumerate(group_cats):\n",
    "  if group_values[idx] < 1000:\n",
    "    remove_cats.append(cat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['cat'] = df['cat'].apply(lambda cat: None if cat in remove_cats else cat)\n",
    "df = df.dropna(subset=['cat'])\n",
    "df = df.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_cats = list(sorted(df['cat'].unique()))\n",
    "print(f'We have #{len(unique_cats)}: {unique_cats}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train,Test split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['cat_id'] = df['cat'].apply(lambda t: unique_cats.index(t))\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=1, stratify=df['cat'])\n",
    "train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['cat'])\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = train['text'].values.tolist(), train['cat_id'].values.tolist()\n",
    "x_valid, y_valid = valid['text'].values.tolist(), valid['cat_id'].values.tolist()\n",
    "x_test, y_test = test['text'].values.tolist(), test['cat_id'].values.tolist()\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}